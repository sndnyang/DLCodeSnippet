{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_set(norm=1):\n",
    "    if norm == 1:\n",
    "        trans = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "    elif norm == 2:\n",
    "        trans = transforms.Compose([\n",
    "            transforms.ToTensor()\n",
    "            # it will automatically do x/255.0 by the transforms\n",
    "            # transforms.Lambda(lambda x: x / 255.0)\n",
    "        ])\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    dir_path = os.path.join(os.environ['HOME'], 'project/data/dataset/mnist')\n",
    "    train_set = datasets.MNIST(dir_path, train=True, download=True, transform=trans)\n",
    "    test_set = datasets.MNIST(dir_path, train=False, transform=trans)\n",
    "    return train_set, test_set, (1, 28, 28), 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class InfiniteSampler(Sampler):\n",
    "    def __init__(self, num_samples, index=None):\n",
    "        # index -> the index of a subset\n",
    "        self.order = index\n",
    "        if self.order is not None:\n",
    "            self.num_samples = self.order.shape[0]\n",
    "        else:\n",
    "            self.num_samples = num_samples\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.loop())\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 ** 31\n",
    "\n",
    "    def loop(self):\n",
    "        # return a sequence of the index\n",
    "        i = 0\n",
    "        if self.order is None:\n",
    "            # first epoch use 0~N-1, easy to compare data.\n",
    "            self.order = np.arange(self.num_samples)\n",
    "        while True:\n",
    "            yield self.order[i]\n",
    "            i += 1\n",
    "            if i >= self.num_samples:\n",
    "                # generate new permutations.\n",
    "                self.order = np.random.permutation(self.order)\n",
    "                i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set, shape, num_classes = mnist_set(norm=2)\n",
    "test_loader = DataLoader(test_set, 128, False)\n",
    "num_examples = len(train_set)\n",
    "\n",
    "size = 1000\n",
    "\n",
    "# the different part from pytorch_sup.py\n",
    "# select a subset, each category has args.size/10 data\n",
    "_train_set_y = train_set.targets.numpy()\n",
    "if size > 50000:\n",
    "    print(\"The numbers of each category in MNIST's training data are not the same\")\n",
    "    raise NotImplementedError\n",
    "ind = np.concatenate([np.where(_train_set_y == i)[0][: int(size / 10)] for i in range(10)])\n",
    "# in-place shuffling\n",
    "np.random.shuffle(ind)\n",
    "train_iter = iter(DataLoader(train_set, 128, num_workers=0, sampler=InfiniteSampler(ind.shape[0], ind)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load_npz_as_dict(path):\n",
    "    data = np.load(path)\n",
    "    return {key: data[key] for key in data}\n",
    "\n",
    "\n",
    "def augmentation(images, random_crop=True, random_flip=True):\n",
    "    # random crop and random flip\n",
    "    h, w = images.shape[2], images.shape[3]\n",
    "    pad_size = 2\n",
    "    aug_images = []\n",
    "    padded_images = np.pad(images, ((0, 0), (0, 0), (pad_size, pad_size), (pad_size, pad_size)), 'reflect')\n",
    "    for image in padded_images:\n",
    "        if random_flip:\n",
    "            image = image[:, :, ::-1] if np.random.uniform() > 0.5 else image\n",
    "        if random_crop:\n",
    "            offset_h = np.random.randint(0, 2 * pad_size)\n",
    "            offset_w = np.random.randint(0, 2 * pad_size)\n",
    "            image = image[:, offset_h:offset_h + h, offset_w:offset_w + w]\n",
    "        else:\n",
    "            image = image[:, pad_size:pad_size + h, pad_size:pad_size + w]\n",
    "        aug_images.append(image)\n",
    "    ret = np.stack(aug_images)\n",
    "    assert ret.shape == images.shape\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _augmentation(images, trans=True, flip=True):\n",
    "    # shape of `image' [N, K, W, H]\n",
    "    assert images.ndim == 4\n",
    "    return augmentation(images, trans, flip)\n",
    "\n",
    "\n",
    "class Data:\n",
    "    # Data Shuffling from vat_chainer\n",
    "    # MNIST won't use it to get batches of data, only needs self.data, self.label\n",
    "    def __init__(self, data, label):\n",
    "        self.data = data\n",
    "        self.label = label\n",
    "        self.index = np.arange(self.N)\n",
    "\n",
    "    @property\n",
    "    def N(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def get(self, n=None, shuffle=True, aug_trans=False, aug_flip=False):\n",
    "        if shuffle:\n",
    "            ind = np.random.permutation(self.data.shape[0])\n",
    "        else:\n",
    "            ind = np.arange(self.data.shape[0])\n",
    "        if n is None:\n",
    "            n = self.data.shape[0]\n",
    "        index = ind[:n]\n",
    "        batch_data = self.data[index]\n",
    "        batch_label = self.label[index]\n",
    "        if aug_trans or aug_flip:\n",
    "            batch_data = _augmentation(batch_data, aug_trans, aug_flip)\n",
    "        return batch_data, batch_label\n",
    "\n",
    "\n",
    "def load_mnist_dataset():\n",
    "    if sys.version_info.major == 3:\n",
    "        dataset = pickle.load(open('dataset/mnist.pkl', 'rb'), encoding=\"bytes\")\n",
    "    else:\n",
    "        dataset = pickle.load(open('dataset/mnist.pkl', 'rb'))\n",
    "    train_set_x = np.concatenate((dataset[0][0], dataset[1][0]), axis=0).astype(\"float32\")\n",
    "    train_set_y = np.concatenate((dataset[0][1], dataset[1][1]), axis=0)\n",
    "    return (train_set_x, train_set_y), (dataset[2][0], dataset[2][1])\n",
    "\n",
    "\n",
    "def load_mnist_for_semi_sup(n_l=1000, n_v=1000):\n",
    "    dataset = load_mnist_dataset()\n",
    "\n",
    "    _train_set_x, _train_set_y = dataset[0]\n",
    "    test_set_x, test_set_y = dataset[1]\n",
    "    test_set_x = test_set_x.reshape(test_set_x.shape[0], 1, 28, 28)\n",
    "\n",
    "    rand_ind = np.random.permutation(_train_set_x.shape[0])  # the seed from outer space can control this one\n",
    "    _train_set_x = _train_set_x[rand_ind]\n",
    "    _train_set_y = _train_set_y[rand_ind]\n",
    "\n",
    "    s_c = int(n_l / 10.0)\n",
    "    train_set_x = np.zeros((n_l, 28 ** 2))\n",
    "    train_set_y = np.zeros(n_l)\n",
    "    for i in range(10):\n",
    "        ind = np.where(_train_set_y == i)[0]\n",
    "        train_set_x[i * s_c:(i + 1) * s_c, :] = _train_set_x[ind[0:s_c], :]\n",
    "        train_set_y[i * s_c:(i + 1) * s_c] = _train_set_y[ind[0:s_c]]\n",
    "        # remove them from the set\n",
    "        _train_set_x = np.delete(_train_set_x, ind[0:s_c], 0)\n",
    "        _train_set_y = np.delete(_train_set_y, ind[0:s_c])\n",
    "\n",
    "    l_rand_ind = np.random.permutation(train_set_x.shape[0])   # shuffle from uniform sequence to random permutation\n",
    "    train_set_x = train_set_x[l_rand_ind].astype(\"float32\").reshape(l_rand_ind.shape[0], 1, 28, 28)\n",
    "    train_set_y = train_set_y[l_rand_ind]\n",
    "\n",
    "    valid_set_x = _train_set_x[:n_v]\n",
    "    valid_set_x = valid_set_x.reshape(n_v, 1, 28, 28)\n",
    "    valid_set_y = _train_set_y[:n_v]\n",
    "    train_set_ul_x = _train_set_x[n_v:]\n",
    "    train_set_ul_x = train_set_ul_x.reshape(train_set_ul_x.shape[0], 1, 28, 28)\n",
    "    train_set_ul_y = _train_set_y[n_v:]\n",
    "    # Will unlabeled set contain labeled points?\n",
    "    # train_set_ul_x = np.concatenate((train_set_x, _train_set_x[n_v:]), axis=0)\n",
    "    # train_set_ul_y = np.concatenate((train_set_y, _train_set_y[n_v:]), axis=0)\n",
    "    # train_set_ul_x = train_set_ul_x[np.random.permutation(train_set_ul_x.shape[0])]\n",
    "    # ul_y is useless\n",
    "    # train_set_ul_y = train_set_ul_y[np.random.permutation(train_set_ul_x.shape[0])]\n",
    "\n",
    "    return (train_set_x, train_set_y), (train_set_ul_x, train_set_ul_y), (valid_set_x, valid_set_y), (test_set_x, test_set_y)\n",
    "\n",
    "\n",
    "def load_dataset(dirpath, size=0):\n",
    "    if 'mnist' in dirpath:\n",
    "        train_l, train_ul, val_set, test_set = load_mnist_for_semi_sup(n_l=size)\n",
    "        train_l = {'images': train_l[0], 'labels': train_l[1]}\n",
    "        train_ul = {'images': train_ul[0], 'labels': train_ul[1]}\n",
    "        # use val_set or test_set\n",
    "        test = {'images': test_set[0], 'labels': test_set[1]}\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return Data(train_l['images'], train_l['labels'].astype(np.int32)), \\\n",
    "           Data(train_ul['images'], train_ul['labels'].astype(np.int32)), \\\n",
    "           Data(test['images'], test['labels'].astype(np.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/mnist.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-80a6692b59ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mnist\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_l\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ce82feedcda1>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(dirpath, size)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'mnist'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mtrain_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_for_semi_sup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mtrain_l\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_l\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mtrain_ul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_ul\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_ul\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ce82feedcda1>\u001b[0m in \u001b[0;36mload_mnist_for_semi_sup\u001b[0;34m(n_l, n_v)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_mnist_for_semi_sup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_v\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mnist_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0m_train_set_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_train_set_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-ce82feedcda1>\u001b[0m in \u001b[0;36mload_mnist_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_mnist_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/mnist.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bytes\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset/mnist.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/mnist.pkl'"
     ]
    }
   ],
   "source": [
    "train_l, train_ul, test_set = load_dataset(\"mnist\", size=100)\n",
    "x, t = train_l.get(128)\n",
    "images = torch.FloatTensor(x)\n",
    "labels = torch.LongTensor(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
